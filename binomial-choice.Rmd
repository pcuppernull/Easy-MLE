---
title: "Binomial Choice"
author: "Pete Cuppernull"
date: "1/4/2021"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(MASS)
```

# Code

### Estimate Model

Create your model here. `y` is your dependent (outcome) variable, which should take values of 0 or 1. This is what we mean by *binomial choice* -- there are two possible outcomes. In the `glm` function (from base `R`), `family = "binomial"` indicates a logit model.

```{r}
model <- glm(y ~ x1 + x2 + x3, data = data, family = "binomial") # where y is the dependent variable; x1, x2, x3 are the independent variables (add as many as you want)
# data is your data frame; `family = "binomial"` indicates that we have a binary outcome.
```

### Set Simulation Values

We are going to use the model we just estimated above to run maximum likelihood simulations. In effect, we plug in interesting values for each for the independent variables to observe the "most likely" dependent variables, per the specification of the model. One common way to do this is by varying one independent variable across a few substantively interesting values while holding all the other variable constant at their means in the data set.

```{r}
x.low <- c(1, # we add 1 here to account for the constant in the model
             min(data$x1, na.rm = TRUE), # this is the independent variable of interest. We start by setting this at its minimum value in the data set
             mean(data$x2, na.rm = TRUE), # for the other independent variables, we set them at their means
             mean(data$x3, na.rm = TRUE)) # for the other independent variables, we set them at their means
x.high <- c(1, # we add 1 here to account for the constant in the model
             max(data$x1, na.rm = TRUE), # now, we set x1 to its maximum value. 
             mean(data$x2, na.rm = TRUE), # for the other independent variables, we set them at their means
             mean(data$x3, na.rm = TRUE)) # for the other independent variables, we set them at their means
```

When we vary `x1` and hold everything else constant, we can observe how to model predictions (i.e. the value of y) varies just by altering `x1`. We call these vectors **x**.low and **x**.high because we are specifying values for our **x** values.

### Create Function for Simulations

Next, we create the function to run our simulations. For each set of `x` values above, this function will run 1,000 simulations. You might ask: why do we need to run multiple simulations for the same `x` values? We run multiple simulations because we also need to account for the variance in the model. Each `x` value we specified will be multiplied not by the coefficients (point estimates) from each `x` in the original model -- rather, they will be multiplied by a value drawn from a normal distribution with the mean set to the coefficient of the original model and variance of the `x` from the model.

Some of the terminology and code below is drawn from Wald and Alquist (2018). Don't worry too much about why we call them "beta", "b.tilde", etc. -- try to focus on the intuition of each step.
```{r}
binomial_fd <- function(model, low_vector, high_vector){
  #Generate Betas -- this is where we draw values from the normal distribution from each simulation. mvrnorm (from the MASS package) draws values
  #from the multivaraite normal distribution. We use the coefficients from the original model and the variance-covariance matrix from the original model.
  b.tilde <- mvrnorm(1000, coef(model), vcov(model))
  
  #inverse logit function - this is our link function used to generate the outcome variable.
  inv.logit <- function (x) 1/(1+exp(-x))
  
  #Run Simulations -- now, we take the dot product of each of the 1,000 draws and the vector of values we specified.
  s.low <- inv.logit(b.tilde %*% low_vector)
  s.high <- inv.logit(b.tilde %*% high_vector)
  
  
  #Extract Probabilities -- mean, SD, and interesting quantile values for the simulations for each x vector
  p.low <- c(mean(s.low), sd(s.low), t(apply(s.low, 2, quantile, c(0.025, .5, .975))))
  p.high <- c(mean(s.high), sd(s.high), t(apply(s.high, 2, quantile, c(0.025, .5, .975))))
  
  #Create Table to Present Results
  table <- as.data.frame(rbind(p.low, p.high)) %>%
  round(3)
    #create row names from the original arguments
    rownames(table) <- c(deparse(substitute(low_vector)), 
                            deparse(substitute(high_vector)))
    colnames(table) <- c("Mean", "SD", "2.5%", "50%", "97.5%")
  
  table
}

binomial_fd(model, x.low, x.high)
```

That's it! The table presents the maximum likelihood estimates for each vector of `x` values you created. The table also displays the 95% confidence interval for the estimates. You can imagine modifying this function to add more vectors of x values -- for each new vector, simply add a new argument, a new `s.` line that takes the new vector, a `p.` line for the new `s.` object, and a new line within the code creating `rownames(table)`.

# Visualization
Coming soon (hopefully)!

# Application
Below, we have some survey data on affect towards Joe Biden. In addition to affect, we have each respondent's sex, age, education level, and binary indicators that indicate part affiliation as Democrat or Republican.

Let's begin by loading in the data. We then create a binary `love_biden` variable, which take a value of 1 if affect towards Biden is 80 or higher, and 0 otherwise.

###Load in Data
```{r}
biden <- read_csv("/Users/petecuppernull/Dropbox/UChicago/2019-20/Winter/Computatonal Modeling/biden.csv")

biden <- biden %>%
  mutate(love_biden = if_else(biden >= 80, 1, 0))
```


#### Model
Next, we create our model, just like we walked through above.
```{r}
# model <- glm(y ~ x1 + x2 + x3, data = data, family = "binomial")

model <- glm(love_biden ~ female + age + educ + dem + rep, data = biden, family = "binomial")
```

#### Set Simulation Values

Let's say we want to understand how education levels affect how likely a person is to "love Biden". I set two `x` vectors below for values of the independent variables. We set the `educ` variable as 12 and 16 in each respective vector, indicating high school and college completion (i.e. total number of years of schooling completed). All other variables are held at their means. With this setup, we can observe how the probability of "loving Biden" changes across each education level for the "average" individual in the data set.   
```{r}
x.hs <- c(1,
            mean(biden$female, na.rm = TRUE),
            mean(biden$age, na.rm = TRUE),
            12,
            mean(biden$dem, na.rm = TRUE),
            mean(biden$rep, na.rm = TRUE))

x.college <- c(1,
            mean(biden$female, na.rm = TRUE),
            mean(biden$age, na.rm = TRUE),
            16,
            mean(biden$dem, na.rm = TRUE),
            mean(biden$rep, na.rm = TRUE))
```


#### First Differences

And now, we just plug the model and each vector into our function!

```{r}
binomial_fd(model, x.hs, x.college)
```

What do we find? Well, the mean probability of "loving Biden" is almost the same between the groups! Remember, this doesn't necessarily mean that education doesn't matter: it means that once we control for things like party affiliation, education levels don't have a very strong effect.

What if we run first differences on party affiliation?

#### Whhat about Party Affiliation?
```{r}
x.dem <- c(1,
            mean(biden$female, na.rm = TRUE),
            mean(biden$age, na.rm = TRUE),
            mean(biden$educ, na.rm = TRUE),
            1, #respondent is a democrat
            0) # respondent is not a republican

x.rep <- c(1,
            mean(biden$female, na.rm = TRUE),
            mean(biden$age, na.rm = TRUE),
            mean(biden$educ, na.rm = TRUE),
            0, #respondent is not a democrat
            1) # respondent is a republican
```

```{r}
binomial_fd(model, x.dem, x.rep)
```
Wow! Now that is a big effect. The "average" respondent in the data set who is a democrat has nearly a 50% chance of "loving Biden". A Republican who otherwise has the same characteristics has only a tiny probability of "loving Biden". This is a more intuitive result.

