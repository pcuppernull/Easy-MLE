---
title: "Binomial Choice"
author: "Pete Cuppernull"
date: "1/4/2021"
output: pdf_document
---

# Load Packages
```{r setup, include=FALSE}
library(tidyverse)
library(stargazer)
library(MASS)
```

# Code

## First Differences

### Estimate Model

Create your model here. `y` is your dependent (outcome) variable, which should take values of 0 or 1. This is what we mean by *binomial choice* -- there are two possible outcomes. In the `glm` function (from base `R`), `family = "binomial"` indicates a logit model.

```{r}
model <- glm(y ~ x1 + x2 + x3, data = data, family = "binomial") # where y is the dependent variable; x1, x2, x3 are the independent variables (add as many as you want)
# data is your data frame; `family = "binomial"` indicates that we have a binary outcome.
```

### Set Simulation Values

We are going to use the model we just estimated above to run maximum likelihood simulations. In effect, we plug in interesting values for each for the independent variables to observe the "most likely" dependent variables, per the specification of the model. One common way to do this is by varying one independent variable across a few substantively interesting values while holding all the other variable constant at their means in the data set.

```{r}
x.low <- c(1, # we add 1 here to account for the constant in the model
             min(data$x1, na.rm = TRUE), # this is the independent variable of interest. We start by setting this at its minimum value in the data set
             mean(data$x2, na.rm = TRUE), # for the other independent variables, we set them at their means
             mean(data$x3, na.rm = TRUE)) # for the other independent variables, we set them at their means
x.high <- c(1, # we add 1 here to account for the constant in the model
             max(data$x1, na.rm = TRUE), # now, we set x1 to its maximum value. 
             mean(data$x2, na.rm = TRUE), # for the other independent variables, we set them at their means
             mean(data$x3, na.rm = TRUE)) # for the other independent variables, we set them at their means
```

When we vary `x1` and hold everything else constant, we can observe how to model predictions (i.e. the value of y) varies just by altering `x1`. We call these vectors **x**.low and **x**.high because we are specifying values for our **x** values.

### Create Function for Simulations

Next, we create the function to run our simulations. For each set of `x` values above, this function will run 1,000 simulations. You might ask: why do we need to run multiple simulations for the same x values? We run multiple simulations because we also need to account for the variance in the model. Each `x` value we specified will be multiplied not by the coefficients (point estimates) from each `x` in the original model -- rather, they will be multiplied by a value drawn from a normal distribution with the mean set to the coefficient of the original model and variance of the `x` from the model.

Some of the terminology below is drawn from Wald and Alquist (2018). Don't worry too much about why we call them "beta", "b.tilde", etc. -- try to focus on the intuition of each step.
```{r}
binomial_fd <- function(model, low_vector, high_vector){
  #Generate Betas -- this is where we draw values from the normal distribution from each simulation. mvrnorm (from the MASS package) draws values
  #from the multivaraite normal distribution. We use the coefficients from the original model and the variance-covariance matrix from the original model.
  b.tilde <- mvrnorm(1000, coef(model), vcov(model))
  
  #inverse logit function - this is our link function used to generate the outcome variable.
  inv.logit <- function (x) 1/(1+exp(-x))
  
  #Run Simulations -- now, we take the dot product of each of the 1,000 draws and the vector of values we specified.
  s.low <- inv.logit(b.tilde %*% low_vector)
  s.high <- inv.logit(b.tilde %*% high_vector)
  
  
  #Extract Probabilities -- mean, SD, and interesting quantile values for the simulations for each x vector
  p.low <- c(mean(s.low), sd(s.low), t(apply(s.low, 2, quantile, c(0.025, .5, .975))))
  p.high <- c(mean(s.high), sd(s.high), t(apply(s.high, 2, quantile, c(0.025, .5, .975))))
  
  #Create Table to Present Results
  table <- as.data.frame(rbind(p.low, p.high)) %>%
  round(3)
    #create row names from the original arguments
    rownames(table) <- c(deparse(substitute(low_vector)), 
                            deparse(substitute(high_vector)))
    colnames(table) <- c("Mean", "SD", "2.5%", "50%", "97.5%")
  
  table
}

binomial_fd(model, x.low, x.high)
```

That's it! The table presents the maximum likelihood estimates for each vector of `x` values you created. The table also displays the 95% confidence interval for the estimates. You can easily modify this function to add more vectors of x values.

## Graphical Simulations

First differences is useful when we want to observe the MLE estimates for two scenarios (i.e. two vectors of `x` values). What if we are interested in a range of values? While you could modify the code above to create lots of `x` vectors and write a function that processes all those vectors, that would be time consuming, error-prone, and not fun. Instead, we'll take advantage of two useful functions -- `expand.grid` and `map_dfr` -- to scale our code above to process as many vectors of `x` values as we want.

### Model

Our model stays the same. Nothing new to do here!

### Set Simulation Values

Here, we will use `expand.grid` to create our simulation values. `expand.grid` creates a table of all possible combinations of `x` values we specify. So, we can vary one independent variable of interest across a wider range of values while holding the other covariates at their means, and `expand.grid` will return a table of all possible values of results.

```{r}
x.expand <- expand.grid(1, # we add 1 here to account for the constant in the model
             seq(min(data$x1, na.rm = TRUE), max(data$x1, na.rm = TRUE), by = 1),# set the range for x1. Here, x values will cover the minimum value in the data set to the maximum, taking steps of 1
             mean(data$x2, na.rm = TRUE), # for the other independent variables, we set them at their means
             mean(data$x3, na.rm = TRUE)) # for the other independent variables, we set them at their means
```

### Create Function for Simulations

We now modify the simulation code from above to process one `x` vector instead of two. After this, our last step will be to iterate this new function over each row of `x` vectors we created above.

```{r}
binomial_range <- function(model, vector){
  #Generate Betas -- this is where we draw values from the normal distribution from each simulation. mvrnorm (from the MASS package) draws values
  #from the multivaraite normal distribution. We use the coefficients from the original model and the variance-covariance matrix from the original model.
  b.tilde <- mvrnorm(1000, coef(model), vcov(model))
  
  #inverse logit function - this is our link function used to generate the outcome variable.
  inv.logit <- function (x) 1/(1+exp(-x))
  
  #Run Simulations -- now, we take the dot product of each of the 1,000 draws and the vector of values.
  s <- inv.logit(b.tilde %*% vector)
  
  
  #Extract Probabilities -- mean, SD, and interesting quantile values for the simulations above
  p <- c(mean(s), sd(s), t(apply(s, 2, quantile, c(0.025, .5, .975))))
  
  #Create Table to Present Results
  table <- as.data.frame(rbind(p)) %>%
  round(3)
    #create row names from the original arguments
    rownames(table) <- c(deparse(substitute(vector)))
    colnames(table) <- c("Mean", "SD", "2.5%", "50%", "97.5%")
  
  table
}
```



